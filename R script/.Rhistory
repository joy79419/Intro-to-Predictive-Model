rm(list=ls())
df<-read.csv("C:/Joey/Intro to Predictive Model/bank-additional/bank-additional-full.csv")
# Check if there is any missing value
apply(df, 2, function(x) any(is.na(x)))
# Descriptive statistics
summary(df)
# Change y (yes to 1, no to 0)
attach(df)
new_y<-ifelse(y=="no",0,1)
df$y<-NULL
df<-cbind(df,new_y)
head(df)
names(df)
age_bins <- cut(age,6,labels=c("17-30", "31-44", "45-57", "58-71", "72-84", "85-98"))
df <- cbind(df, age_bins)
names(df)
library(InformationValue)
#Get all categorical variables
factor_vars <- names(df)[c(2:10,15,22)]
factor_vars
df_iv <- data.frame(VARS=factor_vars, InfmV=numeric(length(factor_vars)), STRENGTH=character(length(factor_vars)),stringsAsFactors = F)
for (i in factor_vars){
df_iv[df_iv$VARS == i, "InfmV"] <-
IV(X=df[,i], Y=y)
df_iv[df_iv$VARS == i, "STRENGTH"] <-
attr(IV(X=df[,i], Y=y), "howgood")
}
# Sort
df_iv <- df_iv[order(-df_iv$InfmV), ]
df_iv
factor_vars <- names(df)[c(2:10,15,22)]
df_iv <- data.frame(VARS=factor_vars, InfmV=numeric(length(factor_vars)), STRENGTH=character(length(factor_vars)),stringsAsFactors = F)
for (i in factor_vars){
df_iv[df_iv$VARS == i, "InfmV"] <-
IV(X=df[,i], Y=new_y)
df_iv[df_iv$VARS == i, "STRENGTH"] <-
attr(IV(X=df[,i], Y=new_y), "howgood")
}
# Sort
df_iv <- df_iv[order(-df_iv$InfmV), ]
df_iv
df_iv$VARS[1:6]
fit1 <- glm(y~., data=df, family=binomial)
summary(fit1)
fit1 <- glm(new_y~., data=df, family=binomial)
summary(fit1)
x <- df_iv$VARS[1:6]
x
func <- paste("new_y~",paste(x,collapse = "+"))
func
x_categorical <- df_iv$VARS[1:6]
df_iv
x2<-names(df)[c(11:14,16:20)]
x2
x2<-names(df)[c(12:14,16:20)]
x2
append(x1,x2
)
x1 <- df_iv$VARS[1:6]
x2<-names(df)[c(12:14,16:20)]
append(x1,x2)
x1 <- df_iv$VARS[1:6]
x2 <- names(df)[c(12:14,16:20)]
x <- append(x1,x2)
func <- paste("new_y~",paste(x,collapse = "+"))
func
x1 <- df_iv$VARS[1:6]
x2 <- names(df)[c(12:14,16:20)]
x <- append(x1,x2)
func <- paste("new_y~",paste(x,collapse = "+"))
fit2 <- glm(as.function(func), data=df, family=binomial)
summary(fit2)
x1 <- df_iv$VARS[1:6]
x2 <- names(df)[c(12:14,16:20)]
x <- append(x1,x2)
func <- paste("new_y~",paste(x,collapse = "+"))
fit2 <- glm(func, data=df, family=binomial)
summary(fit2)
df_iv
x1 <- df_iv$VARS[1:7]
x2 <- names(df)[c(12:14,16:20)]
x <- append(x1,x2)
func <- paste("new_y~",paste(x,collapse = "+"))
fit2 <- glm(func, data=df, family=binomial)
summary(fit2)
x1 <- df_iv$VARS[1:6]
x2 <- names(df)[c(12:14,16:20)]
x <- append(x1,x2)
func <- paste("new_y~",paste(x,collapse = "+"))
fit2 <- glm(func, data=df, family=binomial)
summary(fit2)
cor(job,education)
library(caret)
?createDataPartition
train <- createDataPartition(new_y, p=0.8)
head(train)
class(train)
intrain <- createDataPartition(new_y, p=0.8)
train <- df[intrain]
test <- df[-intrain]
intrain <- createDataPartition(new_y, p=0.8, list=F)
train <- df[intrain]
test <- df[-intrain]
head(train)
head(dfg)
head(df)
data(Sonar)
nrow(train)
?createDataPartition
intrain <- createDataPartition(new_y, p=0.8, list=F)
train <- df[intrain,]
test <- df[-intrain,]
head(train)
nrow(train)
nrow(test)
nrow(test)/nrow(train)
fit1 <- glm(new_y~., data=train, family=binomial)
summary(fit1)
?train
?test
?predict
fit1 <- glm(new_y~., data=train, family=binomial)
summary(fit1)
pred1 <- predict(fit1,newdata = test)
pred1 <- predict(fit1,newdata=test,type="response")
pred1 <- predict(fit1,newdata=test,type="prob")
pred1 <- predict(fit1,newdata=test,type="response")
pred1
?confusionMatrix
intrain <- createDataPartition(new_y, p=0.8, list=F)
train <- df[intrain,]
test <- df[-intrain,]
# Logistic Regression (All Features)
fit1 <- glm(train$new_y~., data=train, family=binomial)
summary(fit1)
pred1 <- predict(fit1,newdata=test,type="response")
confusionMatrix(data=pred, test$new_y)
confusionMatrix(data=pred1, test$new_y)
class(pred1)
class(test$new_y)
confusionMatrix(data=drop(pred1), drop(test$new_y))
table(pred1)
confusionMatrix(data=factor(pred1), factor(test$new_y))
nrow(pred1)
count(pred1)
table(factor(pred1, levels=min(test):max(test)),
factor(test, levels=min(test):max(test)))
train$new_y<-as.factor(train$new_y)
fit1 <- glm(train$new_y~., data=train, family=binomial)
summary(fit1)
pred1 <- predict(fit1,newdata=test,type="response")
confusionMatrix(data=factor(pred1 ), factor(test$new_y))
pred1 <- predict(fit1,newdata=test,type="class")
intrain <- createDataPartition(new_y, p=0.8, list=F)
train <- df[intrain,]
test <- df[-intrain,]
intrain <- createDataPartition(y, p=0.8, list=F)
train <- df[intrain,]
test <- df[-intrain,]
# Logistic Regression (All Features)
fit1 <- glm(train$y~., data=train, family=binomial)
summary(fit1)
pred1 <- predict(fit1,newdata=test,type="response")
confusionMatrix(data=factor(pred1 ), factor(test$new_y))
confusionMatrix(data=factor(pred1 ), factor(test$y))
intrain <- createDataPartition(y, p=0.8, list=F)
train <- df[intrain,]
test <- df[-intrain,]
# Logistic Regression (All Features)
fit1 <- glm(train$y~., data=train, family=binomial)
summary(fit1)
pred1 <- predict(fit1,newdata=test,type="class")
confusionMatrix(data=factor(pred1 ), factor(test$y))
intrain <- createDataPartition(y, p=0.8, list=F)
train <- df[intrain,]
test <- df[-intrain,]
# Logistic Regression (All Features)
fit1 <- glm(train$y~., data=train, family=binomial)
summary(fit1)
pred1 <- predict(fit1,newdata=test,type="response")
confusionMatrix(data=pred1, test$y)
new_y<-as.factor(df$new_y)
intrain <- createDataPartition(new_y, p=0.8, list=F)
train <- df[intrain,]
test <- df[-intrain,]
# Logistic Regression (All Features)
fit1 <- glm(train$new_y~., data=train, family=binomial)
summary(fit1)
pred1 <- predict(fit1,newdata=test,type="response")
confusionMatrix(data=pred1, test$new_y)
new_y<-as.factor(df$new_y)
intrain <- createDataPartition(new_y, p=0.8, list=F)
train <- df[intrain,]
test <- df[-intrain,]
# Logistic Regression (All Features)
fit1 <- glm(train$new_y~., data=train, family=binomial)
summary(fit1)
pred1 <- predict(fit1,newdata=test,type="response")
confusionMatrix(factor(data=pred1), factor(test$new_y))
confusionMatrix(data=factor(pred1), factor(test$new_y))
intrain <- createDataPartition(new_y, p=0.8, list=F)
train <- df[intrain,]
test <- df[-intrain,]
fit1 <- train(train$new_y~., data=train, method=glm,family=binomial)
intrain <- createDataPartition(new_y, p=0.8, list=F)
train <- df[intrain,]
test <- df[-intrain,]
# Logistic Regression (All Features)
fit1 <- train(train$new_y~., data=train, method="glm",family="binomial")
?trainControl
?train
ctrl <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)
fit1 <- train(train$new_y~., data=train, method="glm",family="binomial",trControl = ctrl)
head(train)
ctrl <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)
fit1 <- train(new_y~., data=train, method="glm",family="binomial",trControl = ctrl)
summary(fit1)
pred1 <- predict(fit1,newdata=test,type="response")
pred1 <- predict(fit1,newdata=test)
confusionMatrix(data=pred1, test$new_y)
confusionMatrix(data=factor(pred1), factor(test$new_y))
intrain <- createDataPartition(new_y, p=0.8, list=F)
train <- df[intrain,]
test <- df[-intrain,]
fit1 <- glm(train$new_y~., data=train, family=binomial)
summary(fit1)
pred1 <- predict(fit1,newdata=test,type="response")
pred1
install.packages("boot")
intrain <- createDataPartition(new_y, p=0.8, list=F)
intrain1 <- createDataPartition(new_y, p=0.8, list=F)
intrain==intrain1
fit <- glm(func, data=train, family=binomial)
pred <- predict(fit,newdata=test,type='response')
head(pred)
install.packages("plyr")
install.packages("plyr")
install.packages("plyr")
install.packages("plyr")
library(plyr)
library(plyr)
install.packages("plyr")
install.packages("plyr")
library(plyr)
install.packages("nnet")
##################################################
### Section 1. The Neural Net Model for Numeric Y
###read in the data
zag = read.csv("zagat.csv",header=T)
summary(zag)
##################################################
### Section 1. The Neural Net Model for Numeric Y
###read in the data
zag = read.csv("C:/Joey/Intro to Predictive Modeldatasetzagat.csv",header=T)
##################################################
### Section 1. The Neural Net Model for Numeric Y
###read in the data
zag = read.csv("C:/Joey/Intro to Predictive Modeldataset/zagat.csv",header=T)
##################################################
### Section 1. The Neural Net Model for Numeric Y
###read in the data
zag = read.csv("C:/Joey/Intro to Predictive Model/dataset/zagat.csv",header=T)
summary(zag)
###standardize the x's
minv = rep(0,3)
maxv = rep(0,3)
zagsc = zag
for(i in 1:3) {
minv[i] = min(zag[[i]])
maxv[i] = max(zag[[i]])
zagsc[[i]] = (zag[[i]]-minv[i])/(maxv[i]-minv[i])
}
names(zagsc)
names(zag)
### nn library
library(nnet)
###fit nn with just one x=food
set.seed(99)
znn = nnet(price~food,zagsc,size=3,decay=.1,linout=T)
###get fits, print summary,  and plot fit
fznn = predict(znn,zagsc)
plot(zagsc$food,zagsc$price)
oo = order(zagsc$food)
lines(zagsc$food[oo],fznn[oo],col="red",lwd=2)
abline(lm(price~food,zagsc)$coef)
?nnet
summary(znn) # what does this mean?
library(MASS)
?Boston
names(subset(Boston,select = -chas))
subset(Boston,select = -chas)
